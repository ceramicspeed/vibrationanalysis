{"nbformat_minor": 0, "cells": [{"source": "# Introduction to Machine Learning with `scikit-learn III`", "cell_type": "markdown", "metadata": {}}, {"source": "## <img src='https://az712634.vo.msecnd.net/notebooks/python_course/v1/lightening.png' alt=\"Smiley face\" width=\"42\" height=\"42\" align=\"left\">Learning Objectives\n* * *\n* Become familiar with tools in `sklearn` for evaluating a model\n* Learn what a pipline is in `sklearn` and how to use it\n* Learn and practice a technique for hyperparameter tuning with `GridSerchCV`\n<img src='https://az712634.vo.msecnd.net/notebooks/python_course/v1/ml_process.png' alt=\"Smiley face\" width=\"400\"><br>", "cell_type": "markdown", "metadata": {}}, {"source": "## Evaluating Models", "cell_type": "markdown", "metadata": {}}, {"source": "### Evaluating using metrics\n* <b>Confusion matrix<\/b> - visually inspect quality of a classifier's predictions (more [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)) - very useful to see if a particular class is problematic\n\n<b>Here, we will process some data, classify it with SVM (see [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) for more info), and view the quality of the classification with a confusion matrix.<\/b>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Import model algorithm and data\nfrom sklearn import svm, datasets\n\n# Import splitter\nfrom sklearn.cross_validation import train_test_split\n\n# Import metrics method\nfrom sklearn.metrics import confusion_matrix\n\n# Feature data (X) and labels (y)\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, train_size = 0.90, random_state = 42)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Perform the classification step and run a prediction on test set from above\nclf = svm.SVC(kernel = 'rbf', C = 0.5, gamma = 10)\ny_pred = clf.fit(X_train, y_train).predict(X_test)\n\npd.DataFrame({'Prediction': iris.target_names[y_pred],\n    'Actual': iris.target_names[y_test]})", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Accuracy score\nclf.score(X_test, y_test)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Define a plotting function confusion matrices \n#  (from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(cm, target_names, title = 'The Confusion Matrix', cmap = plt.cm.YlOrRd):\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.tight_layout()\n    \n    # Add feature labels to x and y axes\n    tick_marks = np.arange(len(target_names))\n    plt.xticks(tick_marks, target_names, rotation=45)\n    plt.yticks(tick_marks, target_names)\n    \n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    plt.colorbar()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Numbers in confusion matrix:\n* on-diagonal - counts of points for which the predicted label is equal to the true label\n* off-diagonal - counts of mislabeled points", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "cm = confusion_matrix(y_test, y_pred)\n\n# Actual counts\nprint(cm)\n\n# Visually inpsect how the classifier did of matching predictions to true labels\nplot_confusion_matrix(cm, iris.target_names)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "* <b>Classification reports<\/b> - a text report with important classification metrics (e.g. precision, recall)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from sklearn.metrics import classification_report\n\n# Using the test and prediction sets from above\nprint(classification_report(y_test, y_pred, target_names = iris.target_names))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Modify the code below in a new code cell.\n```python\n# Another example with some toy data - Fill in the blanks\n\ny_test = ['cat', 'dog', 'mouse', 'mouse', 'cat', 'cat']\ny_pred = ['mouse', 'dog', 'cat', 'mouse', 'cat', 'mouse']\n\n# How did our predictor do?\nprint(classification_report(y_test, ___, target_names = ___))\n```", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Try here", "outputs": [], "metadata": {"collapsed": true}}, {"source": "QUICK QUESTION:  Is it better to have too many false positives or too many false negatives?", "cell_type": "markdown", "metadata": {}}, {"source": "### Evaluating Models and Under/Over-Fitting\n* Over-fitting or under-fitting can be visualized as below and tuned as we will see later with `GridSearchCV` paramter tuning\n* A <b>validation curve<\/b> gives one an idea of the relationship of model complexity to model performance.\n* For this examination it would help to understand the idea of the [<b>bias-variance tradeoff<\/b>](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).\n* A <b>learning curve<\/b> helps answer the question of if there is an added benefit to adding more training data to a model.  It is also a tool for investigating whether an estimator is more affected by variance error or bias error.", "cell_type": "markdown", "metadata": {}}, {"source": "QUESTION:  Does a hyperparameter when increased/decreased cause overfitting or underfitting?  What are the implications of those cases?", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "## Search for best parameters and create a pipeline", "cell_type": "markdown", "metadata": {}}, {"source": "### Easy reading...create and use a pipeline", "cell_type": "markdown", "metadata": {}}, {"source": "> <b>Pipelining<\/b> (as an aside to this section)\n* `Pipeline(steps=[...])` - where steps can be a list of processes through which to put data or a dictionary which includes the parameters for each step as values\n* For example, here we do a transformation (SelectKBest) and a classification (SVC) all at once in a pipeline we set up.\n\nSee a full example [here](http://scikit-learn.org/stable/auto_examples/feature_stacker.html)\n\nNote:  If you wish to perform <b>multiple transformations<\/b> in your pipeline try [FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from sklearn.cross_validation import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\n# A feature selection instance\nselection = SelectKBest(chi2, k = 2)\n\n# Classification instance\nclf = SVC(kernel = 'linear')\n\n# Make a pipeline\npipeline = Pipeline([(\"feature selection\", selection), (\"classification\", clf)])\n\n# Train the model\npipeline.fit(X, y)\n\n# Score\npipeline.score(X_test, y_test)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Last, but not least, Searching Parameter Space with `GridSearchCV`", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from sklearn.grid_search import GridSearchCV\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npoly = PolynomialFeatures(include_bias = False)\nlm = LinearRegression()\n\npipeline = Pipeline([(\"polynomial_features\", poly),\n                         (\"linear_regression\", lm)])\n\nparam_grid = dict(polynomial_features__degree = list(range(1, 30, 2)),\n                  linear_regression__normalize = [False, True])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid)\ngrid_search.fit(X, y)\nprint(grid_search.best_params_)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "EXERCISE: Tune hyperparameters C and gamma in a support vector machine, SVC, using `GridSearchCV`\n\nThe parameter definitions are:\n1. C: Penalty parameter C of the error term (e.g. try within the range from 0.01 to 1.0)\n* gamma: Kernel coefficient for 'rbf', 'poly' and 'sigmoid' kernels (e.g. try from 0 to 10)\n\nThe non-`GridSearchCV` approach from earlier looks like:\n\n```python\n# Import model algorithm and data\nfrom sklearn import svm, datasets\n\n# Import splitter\nfrom sklearn.cross_validation import train_test_split\n\n# Import metrics\nfrom sklearn.metrics import confusion_matrix\n\n# Feature data (X) and labels (y)\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, train_size = 0.90, random_state = 42)\n    \n# Perform the classification step and run a prediction on test set from above\nclf = svm.SVC(kernel = 'rbf', C = 0.5, gamma = 10)\ny_pred = clf.fit(X_train, y_train).predict(X_test)\n```", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Code up your solution here...", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Additional Resources\n* Another popular python library, [PyBrain](http://pybrain.org/), includes reinforcement learning\n* For a parallel, out-of-core memory Python tool check out [dask](http://dask.pydata.org/en/latest/) and the learn example.", "cell_type": "markdown", "metadata": {}}, {"source": "Created by a Microsoft Employee.\n\t\nThe MIT License (MIT)<br>\nCopyright (c) 2016", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.1", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}