{"nbformat_minor": 0, "cells": [{"source": "# Introduction to Machine Learning with `scikit-learn I`", "cell_type": "markdown", "metadata": {}}, {"source": "## <img src='https://az712634.vo.msecnd.net/notebooks/python_course/v1/robotguy.png' alt=\"Smiley face\" width=\"42\" height=\"42\" align=\"left\">Learning Objectives\n* * *\n* Go through a crash course in ML and get a high-level understanding of `scikit-learn` module\n* See how to visualize and explore a built-in dataset (no need to collect data this time)\n* Get introduced to some dimensionality reduction tranformations (no need to clean data this time)", "cell_type": "markdown", "metadata": {}}, {"source": "<br><br>Here's my hand-drawn diagram of the machine learning process from which we will roughly follow in the Machine Learning modules.<br>\n\n<img src='https://raw.githubusercontent.com/PythonWorkshop/intro-to-sklearn/master/imgs/ml_process_by_micheleenharris.png' alt=\"Smiley face\" width=\"550\">\n", "cell_type": "markdown", "metadata": {}}, {"source": "## A very brief introduction to scikit-learn (aka `sklearn`)\n\nAs a gentle intro, it is helpful to think of the `sklearn` approach having layers of abstraction.  This famous quote certainly applies:\n\n> Easy reading is damn hard writing, and vice versa. <br>\n--Nathaniel Hawthorne\n\nIn `sklearn`, you'll find you have a common programming choice: to do things very explicitly, e.g. pre-process data one step at a time, perhaps do a transformation like PCA, split data into traning and test sets, define a classifier or learner with desired parameters, train the classifier, use the classifier to predict on a test set and then analyze how good it did.  \n\nA different approach and something `sklearn` offers is to combine some or all of the steps above into a pipeline so to speak.  For instance, one could define a pipeline which does all of these steps at one time and perhaps even pits mutlple learners against one another or does some parameter tuning with a grid search (examples will be shown towards the end).  This is what is meant here by layers of abstraction.\n\nSo, in this particular module, for the most part, we will try to be explicit regarding our process and give some useful tips on options for a more automated or pipelined approach.  Just note, once you've mastered the explicit approaches you might want to explore `sklearn`'s `GridSearchCV` and `Pipeline` classes.", "cell_type": "markdown", "metadata": {}}, {"source": "### `sklearn`'s way of algorithms\n\nHere is `sklearn`'s algorithm diagram - (note, this is not an exhaustive list of model options offered in `sklearn`, but serves as a good algorithm guide).  The interactive version is [here](http://scikit-learn.org/stable/tutorial/machine_learning_map/).\n![sklearn's ML map](https://az712634.vo.msecnd.net/notebooks/python_course/v1/ml_map.png)\n\nAs far as algorithms for learning a model (i.e. running some training data through an algorithm), it's nice to think of them in two different ways (with the help of the [machine learning wikipedia article](https://en.wikipedia.org/wiki/Machine_learning)).  \n\nThe first way of thinking about ML, is by the type of information or **<i>input<\/i>** given to a system.  So, given that criteria there are three classical categories:\n1.  Supervised learning - we get the data and the labels\n2.  Unsupervised learning - only get the data (no labels)\n3.  Reinforcement learning - reward/penalty based information (feedback)\n\nAnother way of categorizing ML approaches, is to think of the desired **<i>output<\/i>**:\n1.  Classification\n2.  Regression\n3.  Clustering\n4.  Density estimation\n5.  Dimensionality reduction\n\nThis second approach (by desired <i>output<\/i>) is how `sklearn` categorizes it's ML algorithms.<br><br>\n", "cell_type": "markdown", "metadata": {}}, {"source": "### The problem solved in supervised learning (e.g. classification, regression)\n\nSupervised learning consists in learning the link between two datasets: the **observed data X** and an **external variable y** that we are trying to predict, usually called \u00e2\u0080\u009ctarget\u00e2\u0080\u009d or \u00e2\u0080\u009clabels\u00e2\u0080\u009d. Most often, y is a 1D array of length n_samples.\n\nAll supervised estimators in `sklearn` implement a `fit(X, y)` method to fit the model and a `predict(X)` method that, given unlabeled observations X, returns the predicted labels y.\n\nCommon algorithms you will use to train a model and then use trying to predict the labels of unknown observations are: <b>classification<\/b> and <b>regression<\/b>.  There are many types of classification and regression (for examples check out the `sklearn` algorithm cheatsheet below).\n\n### The problem solved in <i>un<\/i>supervised learning (e.g. dimensionality reduction, clustering)\n\nIn machine learning, the problem of unsupervised learning is that of trying to find <b>hidden structure<\/b> in unlabeled data.\n\nUnsupervised models have a `fit()`, `transform()` and/or `fit_transform()` in `sklearn`.\n\n### There are some instances where ML is just not needed or appropriate for solving a problem.\nSome examples are pattern matching (e.g. regex), group-by and data mining in general (discovery vs. prediction).\n\n> <b>ML TIP:  ML can only answer 5 questions:<\/b>\n* How much/how many?\n* Which category?\n* Which group?\n* Is it weird?\n* Which action?\n\n<p style=\"text-align:right\"><i>explained well by Brandon Rohrer [here]((https://channel9.msdn.com/blogs/Cloud-and-Enterprise-Premium/Data-Science-for-Rest-of-Us)<\/i><\/p>\n", "cell_type": "markdown", "metadata": {}}, {"source": "#### EXERCISE: Should I use ML or can I get away with something else?\n\n* Looking back at previous years, by what percent did housing prices increase over each decade?<br>\n* Looking back at previous years, and given the relationship between housing prices and mean income in my area, given my income how much will a house be in two years in my area?<br>\n* A vacuum like roomba has to make a decision to vacuum the living room again or return to its base.<br>\n* Is this image a cat or dog?<br>\n* Are orange tabby cats more common than other breeds in Austin, Texas?<br>\n* Using my database on housing prices, group my housing prices by whether or not the house is under 10 miles from a school.<br>\n* What is the weather going to be like tomorrow?<br>\n* What is the purpose of life?", "cell_type": "markdown", "metadata": {}}, {"source": "### Your first model - a quick example\n* This model is an illustration of the patterns you will encounter in `sklearn`\n* Just for fun, we'll perform a multiclass logistic regression on the `iris` dataset\n* `sklearn` comes with many datasets ready-to-go for `sklearn`'s algorithms like the `iris` data set\n* In the next part of this module we'll explore the `iris` dataset in detail\n---\nBelow, notice some methods like *`fit`, `predict` and `predict_proba`*.  Many of the classifiers you'll see will share method names like these.  (Note this is a supervised learning classifier)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Familiar imports\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "from sklearn import datasets\n\n# Other datasets in sklearn have similar \"load\" functions\niris = datasets.load_iris()\n\n# Leave one value out from training set - that will be test later on\nX_train, y_train = iris.data[:-1,:], iris.target[:-1]", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "from sklearn.linear_model import LogisticRegression\n\n# Our model - a multiclass regression\n# FIRST we initialize it with default params or specify some\nlogistic = LogisticRegression()\n\n# Train on iris training set\n# SECOND we give the model some training data\nlogistic.fit(X_train, y_train)\n\n# Place data in array of arrays (1D array -> 2D array w/ 1 row)\nX_test = iris.data[-1,:].reshape(1, -1) # this reshape figures out how many columns\n\n# THIRD we give our model some test data and predict something\ny_predict = logistic.predict(X_test)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "print('Predicted class %s, real class %s' % (\n        y_predict, iris.target[-1]))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "print('Probabilities of membership in each class: %s' % \n      logistic.predict_proba(X_test))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "print('Coefs: %s' % \n      logistic.coef_)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "**Separate steps we saw:** 1) Initialize model with desired parameters, 2) train model, 3) predict something with model, 4) view some attributes of model prediction using a test set.", "cell_type": "markdown", "metadata": {}}, {"source": "Generalized linear models in `sklearn` have these attributes the very least:\n* `fit` and `predict`\n* `coef_` and `intercept_`\n* methods or attributes individual to the model (such as `predict_proba()` above)", "cell_type": "markdown", "metadata": {}}, {"source": "QUICK QUESTION:\n* What would have been good to do before plunging right in to a logistic regression model?", "cell_type": "markdown", "metadata": {}}, {"source": "EXERCISE 1:  Follow the general pattern\n\n* Using a toy dataset (below) run through a similar process with a Ridge Regressor.  For example:\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(alpha = 0.5)\n\n# 1 feature, 10 samples\ndata = np.array(sorted(np.random.randint(1, 10, size = 10)))\ntargets = sorted(np.random.randint(1, 10, size = 10))\n\n```\n\n* Create your own test data, based on what you determine training data, X, to be.\n* What do coefficients mean here (`coef_`)? - remember how to get help on a method or attribute\n* Remember how to create a scatter plot of the original data?  Don't forget plot import:\n```python\n%matplotlib inline\nimport matplotlib.pyplot as plt\n```", "cell_type": "markdown", "metadata": {}}, {"source": "SOLUTION 1", "cell_type": "markdown", "metadata": {}}, {"source": "## (Collect Data), Visualize and Explore\n* Well, the collection has already been done for us and this dataset is included with `sklearn`\n* In reality, many datasets will need to go through a preprocessing and exploratory data analysis step.  `pandas` and `sklearn` have many tools for this.\n\n### The Dataset - Fisher's Irises", "cell_type": "markdown", "metadata": {}}, {"source": "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n**two-dimensional array or matrix**.  The arrays can be\neither ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\nThe size of the array is expected to be `n_samples x n_features`.\n\n- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n  A sample can be a document, a picture, a sound, a video, an astronomical object,\n  a row in database or CSV file,\n  or whatever you can describe with a fixed set of quantitative traits.\n- **n_features:**  The number of features or distinct traits that can be used to describe each\n  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n  discrete-valued in some cases.<br><br>\n\nThe number of features must be fixed in advance. However it can be very high dimensional\n(e.g. millions of features) with most of them being zeros for a given sample. This is a case\nwhere `scipy.sparse` matrices can be useful, in that they are much more memory-efficient than numpy arrays.\n\nIf there are labels or targets, they need to be stored in **one-dimensional arrays or lists**.", "cell_type": "markdown", "metadata": {}}, {"source": "Today we are going to use the <b>`iris`<\/b> dataset which comes with `sklearn`.  It's fairly small as we'll see shortly.", "cell_type": "markdown", "metadata": {}}, {"source": "> <b>Remember our ML TIP:  Ask sharp questions.<\/b><br><br>e.g. What type of flower is this (pictured below) closest to of the three given classes?\n\n<img border=\"0\" alt=\"iris species\" src=\"https://az712634.vo.msecnd.net/notebooks/python_course/v1/iris-setosa.jpg\" width=\"200\">\n<p align=\"right\">from http://www.madlantern.com/photography/wild-iris<\/p>\n\n### Labels (species names/classes):\n<img border=\"0\" alt=\"iris species\" src=\"https://az712634.vo.msecnd.net/notebooks/python_course/v1/irises.png\" width=\"500\" height=\"500\">", "cell_type": "markdown", "metadata": {}}, {"source": "TIP: Commonly, machine learning algorithms will require your data to be standardized or normalized.  In `sklearn` the data must also take on a certain structure.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt", "outputs": [], "metadata": {"collapsed": true}}, {"source": "QUICK QUESTION:\n1.  What do you expect this data set to be if you are trying to recognize an iris species?\n* For our `[n_samples x n_features]` data array, what do you think\n    * the samples are?\n    * the features are?", "cell_type": "markdown", "metadata": {}}, {"source": "Modify the following code in a new code cell below to understand types found in this dataset.\n```python\nfrom sklearn import datasets\n\n# Load data\niris = datasets.___()\n\n# Check type of data matrix.  fill in the blank\nprint(type(iris.___))\n\n# Check type of target array.  fill in the blank\nprint(type(iris.___))\n\n# What other types can we use for the target labels?\n```", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Try here", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Features (aka columns in data)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n# Converting to dataframe for nice printing and use the \"feature_names\" attribute\n\npd.DataFrame({'feature name': iris.feature_names})", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Targets (aka labels)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n# Converting to dataframe for clearer printing\npd.DataFrame({'target name': iris.target_names})", "outputs": [], "metadata": {"collapsed": false}}, {"source": "> `sklearn` TIP: all included datasets for have at least `feature_names` and sometimes `target_names`", "cell_type": "markdown", "metadata": {}}, {"source": "### Get to know the data - explore\n* Features (columns/measurements) are depicted in this diagram:\n<img border=\"0\" alt=\"iris data features\" src=\"https://az712634.vo.msecnd.net/notebooks/python_course/v1/iris_petal_sepal.png\" width=\"200\" height=\"200\">\n\nNext, let's explore:\n* Shape\n* The actual data\n* Summaries", "cell_type": "markdown", "metadata": {}}, {"source": "<b>Shape and representation<b>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\n# How many data points (rows) x how many features (columns)\nprint(iris.data.shape)\nprint(iris.target.shape)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<b>Sneak a peek at data (and a reminder of your `pandas` dataframe methods)<b>", "cell_type": "markdown", "metadata": {}}, {"source": "Target names are in **dataset.target_names**.  For nice printing (and actual useful methods), we can convert to a `pandas` data frame.  Fill in the following blanks:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Convert to pandas df (adding real column names) to use some pandas functions (head, describe...)\niris.df = pd.DataFrame(iris.data, \n                       columns = iris.feature_names)\n\n\n# First few rows\niris.df.head()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<b>Summary statitsics<b>\n* use `describe` method", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Summary stats\niris.df.describe()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "* We don't have to do much with the `iris` dataset.  It has no missing values.  It's already in numpy arrays and has the correct shape for `sklearn`.  However we could try <b>standardization<\/b> and/or <b>normalization<\/b>. (later, we will show one-hot-encoding)", "cell_type": "markdown", "metadata": {}}, {"source": "## Visualize\n* There are many ways to visualize and here's a simple one:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\niris.df = pd.DataFrame(iris.data, \n                       columns = iris.feature_names)\n\niris.df['target'] = iris.target\n\n# A bit of rearrangement for plotting\ndf = iris.df.loc[:, ['sepal length (cm)', 'target']]\n\n# Add an index column which indicates index within a class\ndf['idx'] = list(range(0, 50)) * 3\n\n# Rearrange to be rows of class values rather feature values for a sample\ndf = df.pivot(index = 'idx', columns = 'target')\n\n# Convert back to an array\ndf = np.array(df)\n\n# Plot a boxplot!\nplt.boxplot(df, labels = iris.target_names)\nplt.title('sepal length (cm)')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "**Using `pairplot` from `seaborn` is a quick way to see which features separate out our data**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import seaborn as sb\nsb.pairplot(pd.DataFrame(iris.data, columns = iris.feature_names))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<p>What you might have to do before using a learner in `sklearn`:<\/p>\n1. Non-numerics transformed to numeric (tip: use applymap() method from `pandas`)\n* Fill in missing values\n* Standardization\n* Normalization\n* Encoding categorical features (e.g. one-hot encoding or dummy variables)\n", "cell_type": "markdown", "metadata": {}}, {"source": "### Preprocessing (Bonus Material)", "cell_type": "markdown", "metadata": {}}, {"source": "<b>Features should end up in a numpy.ndarray (hence numeric) and labels in a list or numpy array.<\/b>\n\nData options:\n* Use pre-processed [datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) from scikit-learn\n* [Create your own](http://scikit-learn.org/stable/datasets/index.html#sample-generators)\n* Read from a file\n\nIf you use your own data or \"real-world\" data you will likely have to do some data wrangling and need to leverage `pandas` for some data manipulation.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "#### Standardization - make our data look like a standard Gaussian distribution (commonly needed for `sklearn` learners)", "cell_type": "markdown", "metadata": {}}, {"source": "> FYI: you'll commonly see the data or feature set (ML word for data without it's labels) represented as a capital <b>X<\/b> and the targets or labels (if we have them) represented as a lowercase <b>y<\/b>.  This is because the data is a 2D array or list of lists and the targets are a 1D array or simple list.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Standardization aka scaling\nfrom sklearn import preprocessing, datasets\n\n# Make sure we have iris loaded\niris = datasets.load_iris()\n\n\n# Data and labels (target)\nX, y = iris.data, iris.target\n\n# Scale it to a gaussian distribution\nX_scaled = preprocessing.scale(X)\n\n# How does it look now\npd.DataFrame(X_scaled).head()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# let's just confirm our standardization worked (mean is 0 w/ unit variance)\npd.DataFrame(X_scaled).describe()\n\n# Also could:\n#print(X_scaled.mean(axis = 0))\n#print(X_scaled.std(axis = 0))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "> PRO TIP: To save our standardization and reapply later (say to the test set or some new data), create a transformer object like so:\n\n```python\n\n# Standardize through scaling\nscaler = preprocessing.StandardScaler().fit(X_train)\n\n# Apply to a new dataset (e.g. test set):\nscaler.transform(X_test)\n```", "cell_type": "markdown", "metadata": {}}, {"source": "#### Normalization - scaling samples <i>individually<\/i> to have unit norm (vector length)\n* This type of scaling is really important if doing some downstream transformations and learning (see sklearn docs [here](http://scikit-learn.org/stable/modules/preprocessing.html#normalization) for more) where similarity of pairs of samples is examined\n* A basic intro to normalization and the unit vector can be found [here](http://freetext.org/Introduction_to_Linear_Algebra/Basic_Vector_Operations/Normalization/)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Normalization aka scaling individually to unit norm\nfrom sklearn import preprocessing, datasets\n\n# Make sure we have iris loaded\niris = datasets.load_iris()\n\n# What do we call these variables normally? (standard practice-wise)\n___, ___ = iris.data, iris.target\n\n# Scale it to a unit norm\nX_norm = preprocessing.normalize(X, norm='l1')\n\n# How does it look now\npd.DataFrame(X_norm).tail()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "from IPython.display import display_html\n\n# let's just confirm our standardization worked (mean is 0 w/ unit variance)\ndisplay_html(pd.DataFrame(X_norm).describe())\n\n# cumulative sum of normalized and original data: (uncomment to view)\n# display_html(pd.DataFrame(X_norm.cumsum().reshape(X.shape)).tail())\n# display_html(pd.DataFrame(X).cumsum().tail())\n\n# unit norm (convert to unit vectors) - all row sums should be 1 now\nX_norm.sum(axis = 1)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "> PRO TIP: To save our normalization (like standardization above) and reapply later (say to the test set or some new data), create a transformer object like so:\n\n```python\n# Normalize to unit norm\nnormalizer = preprocessing.Normalizer().fit(X_train)\n\n# Apply to a new dataset (e.g. test set):\nnormalizer.transform(X_test) \n\n```", "cell_type": "markdown", "metadata": {}}, {"source": "##  (Clean Data) and Transform Data", "cell_type": "markdown", "metadata": {}}, {"source": "### Make the learning easier or better  beforehand -  feature reduction/selection/creation\n* SelectKBest\n* PCA\n* One-Hot Encoder", "cell_type": "markdown", "metadata": {}}, {"source": "### Selecting k top scoring features (also dimensionality reduction)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# SelectKBest for selecting top-scoring features\n\nfrom sklearn import datasets\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Our nice, clean data (it's not always going to be this easy) - what's the function to load it?\niris = datasets.___()\nX, y = iris.data, iris.target\n\nprint('Original shape:', X.shape)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Modify the following code in the code cell below to create a new feature consisting of the petal width divided by the sepal width.\n```python\n# Let's add a NEW feature - a ratio of two of the iris measurements.\ndf = pd.DataFrame(X, columns = iris.feature_names)\n\n# New feature is petal width / sepal width\ndf['petal width / sepal width'] = ___ / ___\n\n# Grab feature names + new one\nnew_feature_names = df.columns\nprint('New feature names:', list(new_feature_names))\n\n# We've now added a new column to our data\nX = np.array(df)\n```", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Try here", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Modify the following code in a new code cell to perform feature selection with all features and the newly created feature.\n```python\n# Perform feature selection - Fill in the blanks in a new code cell\n\n#  Input is scoring function (here chi2) to get univariate p-values\n#  and number of top-scoring features (k) - here we get the top 3\ndim_red = SelectKBest(chi2, k = 3)\n\n# Train\ndim_red.___(X, y)\n\n# Use model to transform original data\nX_t = dim_red.transform(___)\n```", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Try here", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Show scores, features selected and new shape\nprint('Scores:', dim_red.scores_)\nprint('New shape:', X_t.shape)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Get back the selected columns\nselected = dim_red.get_support() # boolean values\nselected_names = new_feature_names[selected]\n\nprint('Top k features: ', list(selected_names))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "**Note on scoring function selection in `SelectKBest` tranformations:**\n* For regression - [f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression)\n* For classification - [chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [f_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)\n", "cell_type": "markdown", "metadata": {}}, {"source": "### Principal component analysis (aka PCA)\n* Reduces dimensions (number of features), based on what information explains the most variance (or signal)\n* Considered unsupervised learning\n* Useful for very large feature space (e.g. say the botanist in charge of the iris dataset measured 100 more parts of the flower and thus there were 104 columns instead of 4)\n* More about PCA on wikipedia [here](https://en.wikipedia.org/wiki/Principal_component_analysis)", "cell_type": "markdown", "metadata": {}}, {"source": "Modify this code in a new code cell to perform PCA on the iris dataset.\n```python\n# PCA for dimensionality reduction - Fill in the blanks\n\nfrom sklearn import decomposition\nfrom sklearn import datasets\n\niris = datasets.load_iris()\n\nX, y = iris.data, iris.target\n\n# Perform principal component analysis\n# Selects the number of components such that the amount of variance \n#   that needs to be explained is greater than the percentage specified\npca = decomposition.PCA(0.95)\npca.fit(X)\n\n# Apply pca to data - like SelectKBest above\nX_t = pca.___(X)\n\n# Check the dimensions of the transformed data in X_t\nX_t.___\n```", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Try here", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Grab the first two principle components\nx1, x2 = X_t[:, 0], X_t[:, 1]\n\n# Please don't worry about details of the plotting below \n#  (note: you can get the iris names below from iris.target_names, also in docs)\nc1 = np.array(list('rbg')) # colors\ncolors = c1[y] # y coded by color\nclasses = iris.target_names[y] # y coded by iris name\nfor (i, cla) in enumerate(set(classes)):\n    xc = [p for (j, p) in enumerate(x1) if classes[j] == cla]\n    yc = [p for (j, p) in enumerate(x2) if classes[j] == cla]\n    cols = [c for (j, c) in enumerate(colors) if classes[j] == cla]\n    plt.scatter(xc, yc, c = cols, label = cla)\n    plt.ylabel('Principal Component 2')\n    plt.xlabel('Principal Component 1')\nplt.legend(loc = 4)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### More feature selection methods [here](http://scikit-learn.org/stable/modules/feature_selection.html)", "cell_type": "markdown", "metadata": {}}, {"source": "### One Hot Encoding\n* It's an operation on feature labels - a method of dummying variable\n* Expands the feature space by nature of transform - later this can be processed further with a dimensionality reduction (the dummied variables are now their own features)\n* FYI:  One hot encoding variables is needed for python ML module `tenorflow`\n* Can do this with `pandas` method or a `sklearn` one-hot-encoder system", "cell_type": "markdown", "metadata": {}}, {"source": "#### `pandas` method", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Dummy variables with pandas built-in function\n\nimport pandas as pd\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Convert to dataframe and add a column with iris species name\ndata = pd.DataFrame(X, columns = iris.feature_names)\ndata['target_name'] = iris.target_names[y]\n\ndf = pd.get_dummies(data, prefix = ['target_name'])\ndf.head()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### `sklearn` method", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# OneHotEncoder for dummying variables\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# We encode both our categorical variable and it's labels\nenc = OneHotEncoder()\nlabel_enc = LabelEncoder() # remember the labels here\n\n# Encode labels (can use for discrete numerical values as well)\ndata_label_encoded = label_enc.fit_transform(y)\n\n# Encode and \"dummy\" variables\ndata_feature_one_hot_encoded = enc.fit_transform(y.reshape(-1, 1))\nprint(data_feature_one_hot_encoded.shape)\n\nnum_dummies = data_feature_one_hot_encoded.shape[1]\ndf = pd.DataFrame(data_feature_one_hot_encoded.toarray(), \n                  columns = label_enc.inverse_transform(range(num_dummies)))\n\ndf.head()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Created by a Microsoft Employee.\n\t\nThe MIT License (MIT)<br>\nCopyright (c) 2016", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.1", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}